{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KarAnalytics/code_demos/blob/main/Simple_transformer_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYNIiyOa4Q8o"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4a80p3u4YqT",
    "outputId": "99349fb3-6ff0-4046-dd5d-fd43e3d22a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Real Data: IMDb Movie Reviews\n",
    "# We limit to 10,000 words and 200 words per review for speed\n",
    "vocab_size = 10000\n",
    "maxlen = 200\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "# 2. Define the Transformer Encoder Block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        # Multi-Head Self-Attention\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual connection\n",
    "\n",
    "        # Feed Forward Network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output) # Residual connection\n",
    "\n",
    "# 3. Handle Token + Positional Embedding\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions # Adding position to meaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEY_Gxw-4fKX",
    "outputId": "2d6febdd-3c0b-455e-8804-757826521cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer on IMDb Dataset...\n",
      "Epoch 1/2\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 123ms/step - accuracy: 0.7229 - loss: 0.5048 - val_accuracy: 0.8827 - val_loss: 0.2807\n",
      "Epoch 2/2\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 119ms/step - accuracy: 0.9165 - loss: 0.2126 - val_accuracy: 0.8741 - val_loss: 0.3092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7dddb36db1a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Build the Final Classification Model\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2   # Number of attention heads\n",
    "ff_dim = 32     # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x) # Summarize the sequence\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 5. Train on Real Data\n",
    "print(\"Training Transformer on IMDb Dataset...\")\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCQTbWTf44br",
    "outputId": "0f37eb07-37e2-41c2-e6d9-00b8a04899c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "--- Manual Transformer Results ---\n",
      "Review: this movie was an absolute masterpiece with brilliant acting\n",
      "Prediction: Positive (0.0098)\n",
      "\n",
      "Review: i hated every minute of this film the plot was a total disaster\n",
      "Prediction: Negative (0.9857)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Predict on your Specific Test Reviews\n",
    "test_reviews = [\n",
    "    \"this movie was an absolute masterpiece with brilliant acting\",\n",
    "    \"i hated every minute of this film the plot was a total disaster\"\n",
    "]\n",
    "\n",
    "# We must use the same word index mapping used for training\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    encoded_texts = []\n",
    "    for text in texts:\n",
    "        # Convert words to IMDb indices\n",
    "        tokens = text.lower().split()\n",
    "        sequence = [word_index.get(word, 0) + 3 for word in tokens] # +3 is an IMDb dataset quirk\n",
    "        encoded_texts.append(sequence)\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(encoded_texts, maxlen=maxlen)\n",
    "\n",
    "X_test = preprocess_text(test_reviews)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 5. Output Results\n",
    "print(\"\\n--- Manual Transformer Results ---\")\n",
    "for i, review in enumerate(test_reviews):\n",
    "    sentiment = \"Positive\" if predictions[i][1] > 0.5 else \"Negative\"\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Prediction: {sentiment} ({predictions[i][0]:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4OstxP19QYw",
    "outputId": "ffad434a-e4e8-4481-fb3c-fa70a07dd87e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Record 0 ---\n",
      "Label: 1 (1 = Positive, 0 = Negative)\n",
      "Text: <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved...\n",
      "\n",
      "\n",
      "--- Record 1 ---\n",
      "Label: 0 (1 = Positive, 0 = Negative)\n",
      "Text: <START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Why is the label inversion happening? NOT SURE\n",
    "\n",
    "# 1. Load the dataset (limiting to 10,000 most frequent words)\n",
    "(x_train, y_train), _ = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# 2. Get the word index (dictionary)\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# 3. Create a reverse word index to map integers back to words\n",
    "# We shift by 3 because 0, 1, and 2 are reserved for <PAD>, <START>, and <UNK>\n",
    "reverse_word_index = {value + 3: key for (key, value) in word_index.items()}\n",
    "reverse_word_index[0] = \"<PAD>\"\n",
    "reverse_word_index[1] = \"<START>\"\n",
    "reverse_word_index[2] = \"<UNK>\"\n",
    "reverse_word_index[3] = \"<UNUSED>\"\n",
    "\n",
    "def decode_review(text_ids):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text_ids])\n",
    "\n",
    "# 4. Read and print the first two records\n",
    "for i in range(2):\n",
    "    print(f\"--- Record {i} ---\")\n",
    "    print(f\"Label: {y_train[i]} (1 = Positive, 0 = Negative)\")\n",
    "    print(f\"Text: {decode_review(x_train[i][:50])}...\") # Printing first 50 words\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395,
     "referenced_widgets": [
      "4e0ffc79fe55491d849a6b3ec678bfac",
      "343b001eda094a33aa3b6c9c364e749b",
      "0589149f468d46ea91e27331bc475a67",
      "39469f89a69f440d9fc369932247551a",
      "7839413907ce47e7a41215bc32eee9d8",
      "1f085ac5535c409893fb8d546c3430c5",
      "4fd3dffa5c3b48d182adcc13e57a73b4",
      "941505b812a2445cb06fd8eea7fe1767",
      "e4f9a910f3114a9d863019694c00fb07",
      "159c3ed742314d308ea1c1f6c0b380cb",
      "c350895761f24639b37d2d1beff635ee",
      "d6e03af19ae54ecd893b91f87e0e404d",
      "b674a93f27974fb6a6925589c9ac1402",
      "8fdab3c3882f4e63b099d4c13a838feb",
      "ad3d6d0462724e4b91b2c6be3fc09ea8",
      "de64b062cfa149609bed9f5dbef0ad64",
      "1b2794c65e544278b2c83e8b917dc655",
      "0f40a8d8916a44bbba09bb15dc763224",
      "a7a3a3d4e93346d4ae6bf70970cf99f1",
      "8527e1f9a4e248f0aee2a7976f251bd1",
      "34fc5caeb4e4482e8dec8994046a4980",
      "f8eb35aa07e844d096476e1b2ff9f2d8",
      "7acce0edb0fe471b9cab801d0906ead5",
      "c8ec7e5f097d47e78946148bef2343cd",
      "cbfe3451bd474227826e8c2c112b9589",
      "61144d964ae64629972f35fb71452f3f",
      "ccbc33d3160240f08264083426cccaa7",
      "d73d5be03b3448bc888930a3d217cacb",
      "b13b22badb284e4891910f5ab266fa8d",
      "54936b534cee47729c42a2bb31b0dca7",
      "c638476bcac845aaa7df50ff0a6e4217",
      "34cc83c93a08409db9cdfc61282d47fd",
      "9bd9117551df47609b034dfd891ecfa2",
      "fb7edf260ac2473bae9a255c28373857",
      "319bb06fe287419482c8d090f8481cca",
      "dc28d1308c4d4b41ac1407ec4e12aeab",
      "f2f23c3889e2408bac60e6080ef5eb82",
      "8d5a13cfc59a4878a49d82f37b558b98",
      "74fe9b468f474aad8c3c5f90fc65fc40",
      "447fcc75839c4fb2a865ec4ef90acdc7",
      "b1b337e4797849ef8f130dfa4ec46a9b",
      "76feb143e5524f309e1b501d38bb2b51",
      "af9740555cb64d98ba3e81ce7be99e7f",
      "7a0accb088e342abb6b3a433c921350d"
     ]
    },
    "id": "Xxrm0p_n7TJz",
    "outputId": "21b39516-8d26-49f4-b792-9d35f3011203"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0ffc79fe55491d849a6b3ec678bfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e03af19ae54ecd893b91f87e0e404d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acce0edb0fe471b9cab801d0906ead5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7edf260ac2473bae9a255c28373857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie was an absolute masterpiece with brilliant acting.\n",
      "Result: POSITIVE (Confidence: 0.9999)\n",
      "\n",
      "Review: I hated every minute of this film; the plot was a total disaster.\n",
      "Result: NEGATIVE (Confidence: 0.9998)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Let's do it the easier way with BERT (Note that there is no training involved here)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. The Single-Line Model: Load a pre-trained Transformer (DistilBERT)\n",
    "# This handles tokenization, encoding, and the classification head automatically.\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# 2. Test it on real data (Supervised Learning Inference)\n",
    "test_reviews = [\n",
    "    \"This movie was an absolute masterpiece with brilliant acting.\",\n",
    "    \"I hated every minute of this film; the plot was a total disaster.\"\n",
    "]\n",
    "\n",
    "results = classifier(test_reviews)\n",
    "\n",
    "# 3. Print the results\n",
    "for review, result in zip(test_reviews, results):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Result: {result['label']} (Confidence: {result['score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6ouNKJj7ft9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPFxw2QsD6Grp+NBp73pHrm",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
