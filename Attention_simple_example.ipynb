{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPasQLOVyq+Nu4wLzR01u+u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarAnalytics/code_demos/blob/main/Attention_simple_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OU6KwupaVFgN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Dataset: 100 samples of real text\n",
        "\n",
        "### Try improving data and see how the test prediction changes.\n",
        "data = [\n",
        "    (\"the movie was great\", 1), (\"i loved the acting\", 1), (\"simply amazing\", 1),\n",
        "    (\"a masterpiece of cinema\", 1), (\"really fun and exciting\", 1),\n",
        "    (\"the plot was boring\", 0), (\"terrible acting script\", 0), (\"i hated it\", 0),\n",
        "    (\"waste of time\", 0), (\"it was a bad experience\", 0)\n",
        "] * 10\n",
        "\n",
        "texts = [item[0] for item in data]\n",
        "labels = np.array([item[1] for item in data])\n",
        "\n",
        "# 2. Text Preprocessing\n",
        "max_tokens = 100\n",
        "sequence_length = 5\n",
        "vectorize_layer = layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=sequence_length)\n",
        "vectorize_layer.adapt(texts)\n",
        "X_train = vectorize_layer(texts)\n",
        "\n",
        "# 3. Build Model\n",
        "inputs = layers.Input(shape=(sequence_length,), name=\"input_layer\")\n",
        "embedding = layers.Embedding(max_tokens, 16, name=\"embedding_layer\")(inputs)\n",
        "\n",
        "# Built-in Keras Attention\n",
        "# We set return_attention_scores=True to get the weights for visualization\n",
        "attention_output, weights = layers.Attention(name=\"attention_layer\")(\n",
        "    [embedding, embedding], return_attention_scores=True\n",
        ")\n",
        "\n",
        "# Pool the results and classify\n",
        "flat = layers.GlobalAveragePooling1D()(attention_output)\n",
        "outputs = layers.Dense(1, activation='sigmoid', name=\"sentiment_output\")(flat)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=[outputs, weights])\n",
        "\n",
        "# 4. Use a list for loss. 'None' for the attention weights output.\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=['binary_crossentropy', None],\n",
        "    metrics=['accuracy', None]\n",
        ")\n",
        "\n",
        "# 5. Train\n",
        "print(\"Training model...\")\n",
        "### Try changing the no of epochs and see how the attention in the test data changes.\n",
        "model.fit(X_train, labels, epochs=100, verbose=0)\n",
        "print(\"Training complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNLmvyjzVKyn",
        "outputId": "8099ad5e-ea89-4dcb-f0cd-e024a28ee7a3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Training complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Predict and Interpret\n",
        "test_sentence = [\"the movie was boring\"]\n",
        "X_test = vectorize_layer(test_sentence)\n",
        "prediction, attention_weights = model.predict(X_test)\n",
        "\n",
        "# Map numbers back to words\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "words = [vocab[idx] for idx in X_test[0].numpy() if idx != 0]\n",
        "\n",
        "print(f\"Sentence: '{test_sentence[0]}'\")\n",
        "print(f\"Sentiment: {'Positive' if prediction[0] > 0.5 else 'Negative'} ({prediction[0][0]:.4f})\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Average weights across the query dimension to see overall word importance\n",
        "avg_weights = np.mean(attention_weights[0], axis=0)\n",
        "\n",
        "for word, weight in zip(words, avg_weights):\n",
        "    bar = \"█\" * int(weight * 40)\n",
        "    print(f\"{word:<10} | {weight:.4f} {bar}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cchEYKRaVND2",
        "outputId": "6fd50473-143d-4c06-bafc-62f1427efe8f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Sentence: 'the movie was boring'\n",
            "Sentiment: Negative (0.4836)\n",
            "----------------------------------------\n",
            "the        | 0.1600 ██████\n",
            "movie      | 0.2924 ███████████\n",
            "was        | 0.1225 ████\n",
            "boring     | 0.3162 ████████████\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now try it with a slightly better data:\n",
        "\n",
        "# 50 Positive Samples\n",
        "pos_reviews = [\n",
        "    (\"the movie was great\", 1), (\"i loved the acting\", 1), (\"simply amazing\", 1),\n",
        "    (\"a masterpiece of cinema\", 1), (\"really fun and exciting\", 1), (\"incredible story line\", 1),\n",
        "    (\"absolutely wonderful experience\", 1), (\"the best film ever\", 1), (\"highly recommended movie\", 1),\n",
        "    (\"superb performance by all\", 1), (\"i enjoyed every minute\", 1), (\"brilliant directing\", 1),\n",
        "    (\"a true classic film\", 1), (\"spectacular visuals\", 1), (\"i was impressed\", 1),\n",
        "    (\"pure joy to watch\", 1), (\"it was very touching\", 1), (\"a beautiful story\", 1),\n",
        "    (\"top notch acting\", 1), (\"very entertaining film\", 1), (\"the cinematography was stunning\", 1),\n",
        "    (\"captivating from start to finish\", 1), (\"refreshing and original\", 1), (\"greatest movie of year\", 1),\n",
        "    (\"it made me happy\", 1), (\"an emotional journey\", 1), (\"wonderful cast\", 1),\n",
        "    (\"powerful and moving\", 1), (\"delightful cinema\", 1), (\"i really liked it\", 1),\n",
        "    (\"fantastic plot\", 1), (\"breathtaking scenes\", 1), (\"perfectly executed\", 1),\n",
        "    (\"charming movie\", 1), (\"excellent work\", 1), (\"it was so good\", 1),\n",
        "    (\"lovely film\", 1), (\"bold and brave story\", 1), (\"sweet and funny\", 1),\n",
        "    (\"a must watch\", 1), (\"i felt inspired\", 1), (\"outstanding production\", 1),\n",
        "    (\"very well made\", 1), (\"smart and witty\", 1), (\"uplifting ending\", 1),\n",
        "    (\"cool characters\", 1), (\"impressive quality\", 1), (\"honest and raw\", 1),\n",
        "    (\"magical feeling\", 1), (\"i loved it\", 1)\n",
        "]\n",
        "\n",
        "# 50 Negative Samples\n",
        "neg_reviews = [\n",
        "    (\"the plot was boring\", 0), (\"terrible acting script\", 0), (\"i hated it\", 0),\n",
        "    (\"waste of time\", 0), (\"it was a bad experience\", 0), (\"very dull and slow\", 0),\n",
        "    (\"worst movie ever\", 0), (\"extremely disappointed\", 0), (\"poorly directed\", 0),\n",
        "    (\"the script was weak\", 0), (\"did not like it\", 0), (\"annoying characters\", 0),\n",
        "    (\"boring story\", 0), (\"pathetic attempt at drama\", 0), (\"it was so noisy\", 0),\n",
        "    (\"waste of money\", 0), (\"terrible directing\", 0), (\"ugly visuals\", 0),\n",
        "    (\"i fell asleep\", 0), (\"the ending was bad\", 0), (\"not worth watching\", 0),\n",
        "    (\"failed to impress\", 0), (\"silly plot\", 0), (\"boring and predictable\", 0),\n",
        "    (\"i was very frustrated\", 0), (\"bad script writing\", 0), (\"horrible acting\", 0),\n",
        "    (\"nothing makes sense\", 0), (\"clunky dialogue\", 0), (\"very mediocre\", 0),\n",
        "    (\"disaster of a film\", 0), (\"low quality production\", 0), (\"uninspired film\", 0),\n",
        "    (\"cheesy and cheap\", 0), (\"too long and boring\", 0), (\"i regret watching\", 0),\n",
        "    (\"terrible movie\", 0), (\"complete failure\", 0), (\"badly paced\", 0),\n",
        "    (\"offensive and loud\", 0), (\"pointless story\", 0), (\"awful experience\", 0),\n",
        "    (\"it was painful\", 0), (\"messy plot\", 0), (\"boring and dry\", 0),\n",
        "    (\"zero stars\", 0), (\"worst film of year\", 0), (\"really bad\", 0),\n",
        "    (\"uninteresting and plain\", 0), (\"it was terrible\", 0)\n",
        "]\n",
        "\n",
        "data = pos_reviews + neg_reviews\n",
        "np.random.shuffle(data) # Good practice to shuffle!\n",
        "\n",
        "texts = [item[0] for item in data]\n",
        "labels = np.array([item[1] for item in data])\n",
        "\n",
        "# 2. Text Preprocessing\n",
        "max_tokens = 100\n",
        "sequence_length = 5\n",
        "vectorize_layer = layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=sequence_length)\n",
        "vectorize_layer.adapt(texts)\n",
        "X_train = vectorize_layer(texts)\n",
        "\n",
        "# 3. Build Model\n",
        "inputs = layers.Input(shape=(sequence_length,), name=\"input_layer\")\n",
        "embedding = layers.Embedding(max_tokens, 16, name=\"embedding_layer\")(inputs)\n",
        "\n",
        "# Built-in Keras Attention\n",
        "# We set return_attention_scores=True to get the weights for visualization\n",
        "attention_output, weights = layers.Attention(name=\"attention_layer\")(\n",
        "    [embedding, embedding], return_attention_scores=True\n",
        ")\n",
        "\n",
        "# Pool the results and classify\n",
        "flat = layers.GlobalAveragePooling1D()(attention_output)\n",
        "outputs = layers.Dense(1, activation='sigmoid', name=\"sentiment_output\")(flat)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=[outputs, weights])\n",
        "\n",
        "# 4. Use a list for loss. 'None' for the attention weights output.\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=['binary_crossentropy', None],\n",
        "    metrics=['accuracy', None]\n",
        ")\n",
        "\n",
        "# 5. Train\n",
        "print(\"Training model...\")\n",
        "### Try changing the no of epochs and see how the attention in the test data changes.\n",
        "model.fit(X_train, labels, epochs=100, verbose=0)\n",
        "print(\"Training complete.\\n\")\n",
        "\n",
        "# 6. Predict and Interpret\n",
        "test_sentence = [\"the movie was boring\"]\n",
        "X_test = vectorize_layer(test_sentence)\n",
        "prediction, attention_weights = model.predict(X_test)\n",
        "\n",
        "# Map numbers back to words\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "words = [vocab[idx] for idx in X_test[0].numpy() if idx != 0]\n",
        "\n",
        "print(f\"Sentence: '{test_sentence[0]}'\")\n",
        "print(f\"Sentiment: {'Positive' if prediction[0] > 0.5 else 'Negative'} ({prediction[0][0]:.4f})\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Average weights across the query dimension to see overall word importance\n",
        "avg_weights = np.mean(attention_weights[0], axis=0)\n",
        "\n",
        "for word, weight in zip(words, avg_weights):\n",
        "    bar = \"█\" * int(weight * 40)\n",
        "    print(f\"{word:<10} | {weight:.4f} {bar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDkIl9YJiofW",
        "outputId": "c4f63415-9b92-4347-bc7d-731717ccb450"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Training complete.\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "Sentence: 'the movie was boring'\n",
            "Sentiment: Negative (0.2313)\n",
            "----------------------------------------\n",
            "the        | 0.1720 ██████\n",
            "movie      | 0.1828 ███████\n",
            "was        | 0.1782 ███████\n",
            "boring     | 0.2968 ███████████\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Let us try without the attention layer:\n",
        "\n",
        "# 50 Positive Samples\n",
        "pos_reviews = [\n",
        "    (\"the movie was great\", 1), (\"i loved the acting\", 1), (\"simply amazing\", 1),\n",
        "    (\"a masterpiece of cinema\", 1), (\"really fun and exciting\", 1), (\"incredible story line\", 1),\n",
        "    (\"absolutely wonderful experience\", 1), (\"the best film ever\", 1), (\"highly recommended movie\", 1),\n",
        "    (\"superb performance by all\", 1), (\"i enjoyed every minute\", 1), (\"brilliant directing\", 1),\n",
        "    (\"a true classic film\", 1), (\"spectacular visuals\", 1), (\"i was impressed\", 1),\n",
        "    (\"pure joy to watch\", 1), (\"it was very touching\", 1), (\"a beautiful story\", 1),\n",
        "    (\"top notch acting\", 1), (\"very entertaining film\", 1), (\"the cinematography was stunning\", 1),\n",
        "    (\"captivating from start to finish\", 1), (\"refreshing and original\", 1), (\"greatest movie of year\", 1),\n",
        "    (\"it made me happy\", 1), (\"an emotional journey\", 1), (\"wonderful cast\", 1),\n",
        "    (\"powerful and moving\", 1), (\"delightful cinema\", 1), (\"i really liked it\", 1),\n",
        "    (\"fantastic plot\", 1), (\"breathtaking scenes\", 1), (\"perfectly executed\", 1),\n",
        "    (\"charming movie\", 1), (\"excellent work\", 1), (\"it was so good\", 1),\n",
        "    (\"lovely film\", 1), (\"bold and brave story\", 1), (\"sweet and funny\", 1),\n",
        "    (\"a must watch\", 1), (\"i felt inspired\", 1), (\"outstanding production\", 1),\n",
        "    (\"very well made\", 1), (\"smart and witty\", 1), (\"uplifting ending\", 1),\n",
        "    (\"cool characters\", 1), (\"impressive quality\", 1), (\"honest and raw\", 1),\n",
        "    (\"magical feeling\", 1), (\"i loved it\", 1)\n",
        "]\n",
        "\n",
        "# 50 Negative Samples\n",
        "neg_reviews = [\n",
        "    (\"the plot was boring\", 0), (\"terrible acting script\", 0), (\"i hated it\", 0),\n",
        "    (\"waste of time\", 0), (\"it was a bad experience\", 0), (\"very dull and slow\", 0),\n",
        "    (\"worst movie ever\", 0), (\"extremely disappointed\", 0), (\"poorly directed\", 0),\n",
        "    (\"the script was weak\", 0), (\"did not like it\", 0), (\"annoying characters\", 0),\n",
        "    (\"boring story\", 0), (\"pathetic attempt at drama\", 0), (\"it was so noisy\", 0),\n",
        "    (\"waste of money\", 0), (\"terrible directing\", 0), (\"ugly visuals\", 0),\n",
        "    (\"i fell asleep\", 0), (\"the ending was bad\", 0), (\"not worth watching\", 0),\n",
        "    (\"failed to impress\", 0), (\"silly plot\", 0), (\"boring and predictable\", 0),\n",
        "    (\"i was very frustrated\", 0), (\"bad script writing\", 0), (\"horrible acting\", 0),\n",
        "    (\"nothing makes sense\", 0), (\"clunky dialogue\", 0), (\"very mediocre\", 0),\n",
        "    (\"disaster of a film\", 0), (\"low quality production\", 0), (\"uninspired film\", 0),\n",
        "    (\"cheesy and cheap\", 0), (\"too long and boring\", 0), (\"i regret watching\", 0),\n",
        "    (\"terrible movie\", 0), (\"complete failure\", 0), (\"badly paced\", 0),\n",
        "    (\"offensive and loud\", 0), (\"pointless story\", 0), (\"awful experience\", 0),\n",
        "    (\"it was painful\", 0), (\"messy plot\", 0), (\"boring and dry\", 0),\n",
        "    (\"zero stars\", 0), (\"worst film of year\", 0), (\"really bad\", 0),\n",
        "    (\"uninteresting and plain\", 0), (\"it was terrible\", 0)\n",
        "]\n",
        "\n",
        "data = pos_reviews + neg_reviews\n",
        "np.random.shuffle(data) # Good practice to shuffle!\n",
        "\n",
        "texts = [item[0] for item in data]\n",
        "labels = np.array([item[1] for item in data])\n",
        "\n",
        "# 2. Text Preprocessing\n",
        "max_tokens = 100\n",
        "sequence_length = 5\n",
        "vectorize_layer = layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=sequence_length)\n",
        "vectorize_layer.adapt(texts)\n",
        "X_train = vectorize_layer(texts)\n",
        "\n",
        "# 3. Build Model\n",
        "inputs = layers.Input(shape=(sequence_length,), name=\"input_layer\")\n",
        "embedding = layers.Embedding(max_tokens, 16, name=\"embedding_layer\")(inputs)\n",
        "\n",
        "# Built-in Keras Attention\n",
        "\n",
        "### COMMENTED\n",
        "# attention_output, weights = layers.Attention(name=\"attention_layer\")(\n",
        "#     [embedding, embedding], return_attention_scores=True\n",
        "# )\n",
        "\n",
        "# Pool the results and classify\n",
        "\n",
        "## REPLACED this line to use embedding directly\n",
        "# flat = layers.GlobalAveragePooling1D()(attention_output)\n",
        "flat = layers.GlobalAveragePooling1D()(embedding)\n",
        "\n",
        "outputs = layers.Dense(1, activation='sigmoid', name=\"sentiment_output\")(flat)\n",
        "\n",
        "# 5. Train\n",
        "model.fit(X_train, labels, epochs=100, verbose=0)\n",
        "\n",
        "## Weights not required\n",
        "# model = models.Model(inputs=inputs, outputs=[outputs, weights])\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "### model.compile is simpler without attention layer\n",
        "# 4. Use a list for loss. 'None' for the attention weights output.\n",
        "#model.compile(\n",
        "#    optimizer='adam',\n",
        "#    loss=['binary_crossentropy', None],\n",
        "#    metrics=['accuracy', None]\n",
        "#)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# 5. Train\n",
        "print(\"Training model...\")\n",
        "### Try changing the no of epochs and see how the attention in the test data changes.\n",
        "model.fit(X_train, labels, epochs=100, verbose=0)\n",
        "print(\"Training complete.\\n\")\n",
        "\n",
        "# 6. Predict and Interpret\n",
        "test_sentence = [\"the movie was boring\"]\n",
        "X_test = vectorize_layer(test_sentence)\n",
        "\n",
        "## Since we do not have attention_weights, this line also changes\n",
        "## prediction, attention_weights = model.predict(X_test)\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "# Map numbers back to words\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "words = [vocab[idx] for idx in X_test[0].numpy() if idx != 0]\n",
        "\n",
        "print(f\"Sentence: '{test_sentence[0]}'\")\n",
        "print(f\"Sentiment: {'Positive' if prediction[0] > 0.5 else 'Negative'} ({prediction[0][0]:.4f})\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "### ATTENTION WEIGHTS NOT AVAILABLE FOR THIS MODEL\n",
        "# Average weights across the query dimension to see overall word importance\n",
        "#avg_weights = np.mean(attention_weights[0], axis=0)\n",
        "\n",
        "#for word, weight in zip(words, avg_weights):\n",
        "#    bar = \"█\" * int(weight * 40)\n",
        "#    print(f\"{word:<10} | {weight:.4f} {bar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j22cqj7Lk0F1",
        "outputId": "cd9efb58-a2d0-4c26-bfe5-7bbdee95fe40"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Training complete.\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "Sentence: 'the movie was boring'\n",
            "Sentiment: Negative (0.3076)\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}