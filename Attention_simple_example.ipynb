{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME3gQcp1MmUv6D0dD5e8LL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarAnalytics/code_demos/blob/main/Attention_simple_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OU6KwupaVFgN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Dataset: 100 samples of real text\n",
        "\n",
        "### Try improving data and see how the test prediction changes.\n",
        "data = [\n",
        "    (\"the movie was great\", 1), (\"i loved the acting\", 1), (\"simply amazing\", 1),\n",
        "    (\"a masterpiece of cinema\", 1), (\"really fun and exciting\", 1),\n",
        "    (\"the plot was boring\", 0), (\"terrible acting script\", 0), (\"i hated it\", 0),\n",
        "    (\"waste of time\", 0), (\"it was a bad experience\", 0)\n",
        "] * 10\n",
        "\n",
        "texts = [item[0] for item in data]\n",
        "labels = np.array([item[1] for item in data])\n",
        "\n",
        "# 2. Text Preprocessing\n",
        "max_tokens = 100\n",
        "sequence_length = 5\n",
        "vectorize_layer = layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=sequence_length)\n",
        "vectorize_layer.adapt(texts)\n",
        "X_train = vectorize_layer(texts)\n",
        "\n",
        "# 3. Build Model\n",
        "inputs = layers.Input(shape=(sequence_length,), name=\"input_layer\")\n",
        "embedding = layers.Embedding(max_tokens, 16, name=\"embedding_layer\")(inputs)\n",
        "\n",
        "# Built-in Keras Attention\n",
        "# We set return_attention_scores=True to get the weights for visualization\n",
        "attention_output, weights = layers.Attention(name=\"attention_layer\")(\n",
        "    [embedding, embedding], return_attention_scores=True\n",
        ")\n",
        "\n",
        "# Pool the results and classify\n",
        "flat = layers.GlobalAveragePooling1D()(attention_output)\n",
        "outputs = layers.Dense(1, activation='sigmoid', name=\"sentiment_output\")(flat)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=[outputs, weights])\n",
        "\n",
        "# 4. Use a list for loss. 'None' for the attention weights output.\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=['binary_crossentropy', None],\n",
        "    metrics=['accuracy', None]\n",
        ")\n",
        "\n",
        "# 5. Train\n",
        "print(\"Training model...\")\n",
        "### Try changing the no of epochs and see how the attention in the test data changes.\n",
        "model.fit(X_train, labels, epochs=50, verbose=0)\n",
        "print(\"Training complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNLmvyjzVKyn",
        "outputId": "2b220a23-7305-4796-b04c-f0ff1b204f7e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Training complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Predict and Interpret\n",
        "test_sentence = [\"the movie was boring\"]\n",
        "X_test = vectorize_layer(test_sentence)\n",
        "prediction, attention_weights = model.predict(X_test)\n",
        "\n",
        "# Map numbers back to words\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "words = [vocab[idx] for idx in X_test[0].numpy() if idx != 0]\n",
        "\n",
        "print(f\"Sentence: '{test_sentence[0]}'\")\n",
        "print(f\"Sentiment: {'Positive' if prediction[0] > 0.5 else 'Negative'} ({prediction[0][0]:.4f})\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Average weights across the query dimension to see overall word importance\n",
        "avg_weights = np.mean(attention_weights[0], axis=0)\n",
        "\n",
        "for word, weight in zip(words, avg_weights):\n",
        "    bar = \"█\" * int(weight * 40)\n",
        "    print(f\"{word:<10} | {weight:.4f} {bar}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cchEYKRaVND2",
        "outputId": "6a4f509c-5336-413d-8b19-1aaa641229f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "Sentence: 'the movie was boring'\n",
            "Sentiment: Positive (0.5419)\n",
            "----------------------------------------\n",
            "the        | 0.1406 █████\n",
            "movie      | 0.3144 ████████████\n",
            "was        | 0.1271 █████\n",
            "boring     | 0.3090 ████████████\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now try it with a slightly better data:\n",
        "\n",
        "# 50 Positive Samples\n",
        "pos_reviews = [\n",
        "    (\"the movie was great\", 1), (\"i loved the acting\", 1), (\"simply amazing\", 1),\n",
        "    (\"a masterpiece of cinema\", 1), (\"really fun and exciting\", 1), (\"incredible story line\", 1),\n",
        "    (\"absolutely wonderful experience\", 1), (\"the best film ever\", 1), (\"highly recommended movie\", 1),\n",
        "    (\"superb performance by all\", 1), (\"i enjoyed every minute\", 1), (\"brilliant directing\", 1),\n",
        "    (\"a true classic film\", 1), (\"spectacular visuals\", 1), (\"i was impressed\", 1),\n",
        "    (\"pure joy to watch\", 1), (\"it was very touching\", 1), (\"a beautiful story\", 1),\n",
        "    (\"top notch acting\", 1), (\"very entertaining film\", 1), (\"the cinematography was stunning\", 1),\n",
        "    (\"captivating from start to finish\", 1), (\"refreshing and original\", 1), (\"greatest movie of year\", 1),\n",
        "    (\"it made me happy\", 1), (\"an emotional journey\", 1), (\"wonderful cast\", 1),\n",
        "    (\"powerful and moving\", 1), (\"delightful cinema\", 1), (\"i really liked it\", 1),\n",
        "    (\"fantastic plot\", 1), (\"breathtaking scenes\", 1), (\"perfectly executed\", 1),\n",
        "    (\"charming movie\", 1), (\"excellent work\", 1), (\"it was so good\", 1),\n",
        "    (\"lovely film\", 1), (\"bold and brave story\", 1), (\"sweet and funny\", 1),\n",
        "    (\"a must watch\", 1), (\"i felt inspired\", 1), (\"outstanding production\", 1),\n",
        "    (\"very well made\", 1), (\"smart and witty\", 1), (\"uplifting ending\", 1),\n",
        "    (\"cool characters\", 1), (\"impressive quality\", 1), (\"honest and raw\", 1),\n",
        "    (\"magical feeling\", 1), (\"i loved it\", 1)\n",
        "]\n",
        "\n",
        "# 50 Negative Samples\n",
        "neg_reviews = [\n",
        "    (\"the plot was boring\", 0), (\"terrible acting script\", 0), (\"i hated it\", 0),\n",
        "    (\"waste of time\", 0), (\"it was a bad experience\", 0), (\"very dull and slow\", 0),\n",
        "    (\"worst movie ever\", 0), (\"extremely disappointed\", 0), (\"poorly directed\", 0),\n",
        "    (\"the script was weak\", 0), (\"did not like it\", 0), (\"annoying characters\", 0),\n",
        "    (\"boring story\", 0), (\"pathetic attempt at drama\", 0), (\"it was so noisy\", 0),\n",
        "    (\"waste of money\", 0), (\"terrible directing\", 0), (\"ugly visuals\", 0),\n",
        "    (\"i fell asleep\", 0), (\"the ending was bad\", 0), (\"not worth watching\", 0),\n",
        "    (\"failed to impress\", 0), (\"silly plot\", 0), (\"boring and predictable\", 0),\n",
        "    (\"i was very frustrated\", 0), (\"bad script writing\", 0), (\"horrible acting\", 0),\n",
        "    (\"nothing makes sense\", 0), (\"clunky dialogue\", 0), (\"very mediocre\", 0),\n",
        "    (\"disaster of a film\", 0), (\"low quality production\", 0), (\"uninspired film\", 0),\n",
        "    (\"cheesy and cheap\", 0), (\"too long and boring\", 0), (\"i regret watching\", 0),\n",
        "    (\"terrible movie\", 0), (\"complete failure\", 0), (\"badly paced\", 0),\n",
        "    (\"offensive and loud\", 0), (\"pointless story\", 0), (\"awful experience\", 0),\n",
        "    (\"it was painful\", 0), (\"messy plot\", 0), (\"boring and dry\", 0),\n",
        "    (\"zero stars\", 0), (\"worst film of year\", 0), (\"really bad\", 0),\n",
        "    (\"uninteresting and plain\", 0), (\"it was terrible\", 0)\n",
        "]\n",
        "\n",
        "data = pos_reviews + neg_reviews\n",
        "np.random.shuffle(data) # Good practice to shuffle!\n",
        "\n",
        "texts = [item[0] for item in data]\n",
        "labels = np.array([item[1] for item in data])\n",
        "\n",
        "# 2. Text Preprocessing\n",
        "max_tokens = 100\n",
        "sequence_length = 5\n",
        "vectorize_layer = layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=sequence_length)\n",
        "vectorize_layer.adapt(texts)\n",
        "X_train = vectorize_layer(texts)\n",
        "\n",
        "# 3. Build Model\n",
        "inputs = layers.Input(shape=(sequence_length,), name=\"input_layer\")\n",
        "embedding = layers.Embedding(max_tokens, 16, name=\"embedding_layer\")(inputs)\n",
        "\n",
        "# Built-in Keras Attention\n",
        "# We set return_attention_scores=True to get the weights for visualization\n",
        "attention_output, weights = layers.Attention(name=\"attention_layer\")(\n",
        "    [embedding, embedding], return_attention_scores=True\n",
        ")\n",
        "\n",
        "# Pool the results and classify\n",
        "flat = layers.GlobalAveragePooling1D()(attention_output)\n",
        "outputs = layers.Dense(1, activation='sigmoid', name=\"sentiment_output\")(flat)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=[outputs, weights])\n",
        "\n",
        "# 4. Use a list for loss. 'None' for the attention weights output.\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=['binary_crossentropy', None],\n",
        "    metrics=['accuracy', None]\n",
        ")\n",
        "\n",
        "# 5. Train\n",
        "print(\"Training model...\")\n",
        "### Try changing the no of epochs and see how the attention in the test data changes.\n",
        "model.fit(X_train, labels, epochs=100, verbose=0)\n",
        "print(\"Training complete.\\n\")\n",
        "\n",
        "# 6. Predict and Interpret\n",
        "test_sentence = [\"the movie was boring\"]\n",
        "X_test = vectorize_layer(test_sentence)\n",
        "prediction, attention_weights = model.predict(X_test)\n",
        "\n",
        "# Map numbers back to words\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "words = [vocab[idx] for idx in X_test[0].numpy() if idx != 0]\n",
        "\n",
        "print(f\"Sentence: '{test_sentence[0]}'\")\n",
        "print(f\"Sentiment: {'Positive' if prediction[0] > 0.5 else 'Negative'} ({prediction[0][0]:.4f})\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Average weights across the query dimension to see overall word importance\n",
        "avg_weights = np.mean(attention_weights[0], axis=0)\n",
        "\n",
        "for word, weight in zip(words, avg_weights):\n",
        "    bar = \"█\" * int(weight * 40)\n",
        "    print(f\"{word:<10} | {weight:.4f} {bar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDkIl9YJiofW",
        "outputId": "45eda6df-ec68-4943-e386-575bd9f886ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a669bd81940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "Sentence: 'the movie was boring'\n",
            "Sentiment: Negative (0.2192)\n",
            "----------------------------------------\n",
            "the        | 0.1735 ██████\n",
            "movie      | 0.1833 ███████\n",
            "was        | 0.1725 ██████\n",
            "boring     | 0.3013 ████████████\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLustby5irCV"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}