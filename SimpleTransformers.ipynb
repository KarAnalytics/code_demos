{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KarAnalytics/code_demos/blob/main/SimpleTransformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AYNIiyOa4Q8o"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4a80p3u4YqT",
    "outputId": "3bccf9eb-d0fb-49c7-e763-5127ea354296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Real Data: IMDb Movie Reviews\n",
    "# We limit to 10,000 words and 200 words per review for speed\n",
    "vocab_size = 10000\n",
    "maxlen = 200\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "# 2. Define the Transformer Encoder Block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        # Multi-Head Self-Attention\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual connection\n",
    "\n",
    "        # Feed Forward Network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output) # Residual connection\n",
    "\n",
    "# 3. Handle Token + Positional Embedding\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions # Adding position to meaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEY_Gxw-4fKX",
    "outputId": "7e2cc1e9-481e-41c3-9a12-c2adadd6c68a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer on IMDb Dataset...\n",
      "Epoch 1/2\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 15ms/step - accuracy: 0.7302 - loss: 0.5112 - val_accuracy: 0.8817 - val_loss: 0.2837\n",
      "Epoch 2/2\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9225 - loss: 0.2038 - val_accuracy: 0.8693 - val_loss: 0.3114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7a31c9860770>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Build the Final Classification Model\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2   # Number of attention heads\n",
    "ff_dim = 32     # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x) # Summarize the sequence\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 5. Train on Real Data\n",
    "print(\"Training Transformer on IMDb Dataset...\")\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCQTbWTf44br",
    "outputId": "ad544d69-9139-4d42-ae84-3454a624bce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "--- Manual Transformer Results ---\n",
      "Review: this movie was an absolute masterpiece with brilliant acting\n",
      "Prediction: Positive (0.0042)\n",
      "\n",
      "Review: i hated every minute of this film the plot was a total disaster\n",
      "Prediction: Negative (0.9442)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Predict on your Specific Test Reviews\n",
    "test_reviews = [\n",
    "    \"this movie was an absolute masterpiece with brilliant acting\",\n",
    "    \"i hated every minute of this film the plot was a total disaster\"\n",
    "]\n",
    "\n",
    "# We must use the same word index mapping used for training\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    encoded_texts = []\n",
    "    for text in texts:\n",
    "        # Convert words to IMDb indices\n",
    "        tokens = text.lower().split()\n",
    "        sequence = [word_index.get(word, 0) + 3 for word in tokens] # +3 is an IMDb dataset quirk\n",
    "        encoded_texts.append(sequence)\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(encoded_texts, maxlen=maxlen)\n",
    "\n",
    "X_test = preprocess_text(test_reviews)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 5. Output Results\n",
    "print(\"\\n--- Manual Transformer Results ---\")\n",
    "for i, review in enumerate(test_reviews):\n",
    "    sentiment = \"Positive\" if predictions[i][1] > 0.5 else \"Negative\"\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Prediction: {sentiment} ({predictions[i][0]:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiv_N1sPrRXY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. Load the dataset (limiting to 10,000 most frequent words)\n",
    "(x_train, y_train), _ = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# 2. Get the word index (dictionary)\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# 3. Create a reverse word index to map integers back to words\n",
    "# We shift by 3 because 0, 1, and 2 are reserved for <PAD>, <START>, and <UNK>\n",
    "reverse_word_index = {value + 3: key for (key, value) in word_index.items()}\n",
    "reverse_word_index[0] = \"<PAD>\"\n",
    "reverse_word_index[1] = \"<START>\"\n",
    "reverse_word_index[2] = \"<UNK>\"\n",
    "reverse_word_index[3] = \"<UNUSED>\"\n",
    "\n",
    "def decode_review(text_ids):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text_ids])\n",
    "\n",
    "# Read and print the 5th to 10th records\n",
    "# Python uses 0-based indexing, so the 5th record is index 4, and the 10th is index 9.\n",
    "# The range function is exclusive of the stop value, so range(4, 10) will include indices 4, 5, 6, 7, 8, 9.\n",
    "print(\"Querying records from 5th to 10th:\")\n",
    "for i in range(4, 10):\n",
    "    print(f\"--- Record {i+1} ---\") # Displaying record number from 1-based perspective\n",
    "    print(f\"Label: {y_train[i]} (1 = Positive, 0 = Negative)\")\n",
    "    print(f\"Text: {decode_review(x_train[i][:50])}...\") # Printing first 50 words\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4OstxP19QYw",
    "outputId": "c80a482e-3bb5-482a-fcf8-1396195f47d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Record 4 ---\n",
      "Label: 0 (1 = Positive, 0 = Negative)\n",
      "Text: <START> worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still...\n",
      "\n",
      "\n",
      "--- Record 5 ---\n",
      "Label: 0 (1 = Positive, 0 = Negative)\n",
      "Text: <START> begins better than it ends funny that the russian submarine crew <UNK> all other actors it's like those scenes where documentary shots br br spoiler part the message <UNK> was contrary to the whole story it just does not <UNK> br br...\n",
      "\n",
      "\n",
      "--- Record 6 ---\n",
      "Label: 1 (1 = Positive, 0 = Negative)\n",
      "Text: <START> lavish production values and solid performances in this straightforward adaption of jane <UNK> satirical classic about the marriage game within and between the classes in <UNK> 18th century england northam and paltrow are a <UNK> mixture as friends who must pass through <UNK> and lies to discover that they...\n",
      "\n",
      "\n",
      "--- Record 7 ---\n",
      "Label: 0 (1 = Positive, 0 = Negative)\n",
      "Text: <START> the <UNK> tells the story of the four hamilton siblings teenager francis <UNK> <UNK> twins <UNK> joseph <UNK> <UNK> <UNK> <UNK> the <UNK> david samuel who is now the surrogate parent in charge the <UNK> move house a lot <UNK> is unsure why is unhappy with the way things...\n",
      "\n",
      "\n",
      "--- Record 8 ---\n",
      "Label: 1 (1 = Positive, 0 = Negative)\n",
      "Text: <START> just got out and cannot believe what a brilliant documentary this is rarely do you walk out of a movie theater in such awe and <UNK> lately movies have become so over hyped that the thrill of discovering something truly special and unique rarely happens <UNK> <UNK> did this...\n",
      "\n",
      "\n",
      "--- Record 9 ---\n",
      "Label: 0 (1 = Positive, 0 = Negative)\n",
      "Text: <START> this movie has many problem associated with it that makes it come off like a low budget class project from someone in film school i have to give it credit on its <UNK> though many times throughout the movie i found myself laughing hysterically it was so bad at...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Why is the label inversion happening? NOT SURE\n",
    "\n",
    "# 1. Load the dataset (limiting to 10,000 most frequent words)\n",
    "(x_train, y_train), _ = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# 2. Get the word index (dictionary)\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# 3. Create a reverse word index to map integers back to words\n",
    "# We shift by 3 because 0, 1, and 2 are reserved for <PAD>, <START>, and <UNK>\n",
    "reverse_word_index = {value + 3: key for (key, value) in word_index.items()}\n",
    "reverse_word_index[0] = \"<PAD>\"\n",
    "reverse_word_index[1] = \"<START>\"\n",
    "reverse_word_index[2] = \"<UNK>\"\n",
    "reverse_word_index[3] = \"<UNUSED>\"\n",
    "\n",
    "def decode_review(text_ids):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text_ids])\n",
    "\n",
    "# 4. Read and print few sample records\n",
    "# for i in range(2):\n",
    "for i in range(4,10):\n",
    "    print(f\"--- Record {i} ---\")\n",
    "    print(f\"Label: {y_train[i]} (1 = Positive, 0 = Negative)\")\n",
    "    print(f\"Text: {decode_review(x_train[i][:50])}...\") # Printing first 50 words\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427,
     "referenced_widgets": [
      "9b40b07cdeb440a1a1a5edb7c4278b8e",
      "c9e60d62291947089daa592c88991563",
      "892bae8b262f4e5e85a4ac9a5d1accac",
      "83855c73850e4c84a8a5faec64e75236",
      "ebab4218025d43a8b1595a9213789b93",
      "fd33284a98d04015a9212d403ffa0297",
      "28eee19b71f3434fac96dad8facb450c",
      "c59f962fec4e445e9af1effa2971e362",
      "b295e65be775471eae97b84f0ebf5685",
      "1bd15e03a51f46838372f7bb0ef72c00",
      "71cb54bc37d2491c8b280e2abe1c1249",
      "2d6a10ec295f43839139dec39111ea40",
      "13408c3e1d8a4e58a51f583d6b6ed03e",
      "227f77cd044e467bb58a4dd432867ce7",
      "0f288846d7cd46d2b645cc41a6cd262a",
      "4bfc4d48dd254a31939ec556ed09140c",
      "6e3a57aac17648358e36f31850c4906c",
      "79c120e09e6d4415b802eae0581a2e4c",
      "3b36f58bd64444faa504ddb24f1d606e",
      "ecfb32adf11341cdb304ff8d06fead5e",
      "357735a00cc84476bf7592d8c75ef818",
      "5e88ad3d700d4da6970ca77e06e63fb3",
      "7a7ca50a4995447685ea24cfe42cf3d1",
      "3771f6275c504dcd9a73252435cac49f",
      "dab1c545843e4570b74a947532598038",
      "195f9a2eed294b30992d4c4f43ee141f",
      "2ca726158ae14aa7b35b7ed1d1943c23",
      "db9de48e1bc44114ba6b4efb1cdae322",
      "697d731fbf1845f49be94b963be19095",
      "5c136925c0614a87a11c4e0decfeed17",
      "d7f523042dca4e46b995678d921437d1",
      "2457643d5b7d4008929cc75e1c4070f2",
      "28ae23a3bffe45f98edc9d6d170db2d6",
      "f102056a8f2c47d9ad9d349874309204",
      "3cf15ae262ed43afa8be1da33da1ef0f",
      "fd5be880d54e4d61b796f80553596fd5",
      "ffdd3a5ac4f24e43ae4d78dc7d0ce146",
      "1d654d5100b643debc4b45e2f0b20a73",
      "8b4f8279304f4af6a056bf518465c5f9",
      "d10bfb03613243c6b16dd805d1e6dcc6",
      "f7cd59741ddc46e3b922afc6e039169c",
      "81343cd6d7624959bc0e6e7a0c66b8cc",
      "461c3e8bde5e461ebbaefe86f8acac1c",
      "b7ec2eb222094fdeac2d88b2369d8252",
      "ad1a2134d2964a5b80e2bd44ce53bb3c",
      "eeae8b08886642fb8ead3b639a8af826",
      "8e933f7de1d744c78607f332cdb60f5a",
      "224a1e89132d415486e8c275aaf61bee",
      "99542b703b284c098e21e286686e2be2",
      "7e206e6fd0d64655ad0671c1f3186fc2",
      "e864e6a985c946fd81ffb7e7c3aa2192",
      "d10a838e673c472b93b717e6e553611f",
      "be069ffe1c8b465399ac84795c85c2f6",
      "86e93017e5a540cc91f1fff1e0ee8e70",
      "b2c7b780bb864723a0893ac2c4b9b05f"
     ]
    },
    "id": "Xxrm0p_n7TJz",
    "outputId": "5f698c8b-f594-4eaa-bc77-5917ec495a68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b40b07cdeb440a1a1a5edb7c4278b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6a10ec295f43839139dec39111ea40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7ca50a4995447685ea24cfe42cf3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f102056a8f2c47d9ad9d349874309204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1a2134d2964a5b80e2bd44ce53bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie was an absolute masterpiece with brilliant acting.\n",
      "Result: POSITIVE (Confidence: 0.9999)\n",
      "\n",
      "Review: I hated every minute of this film; the plot was a total disaster.\n",
      "Result: NEGATIVE (Confidence: 0.9998)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Let's do it the easier way with BERT (Note that there is no training involved here)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. The Single-Line Model: Load a pre-trained Transformer (DistilBERT)\n",
    "# This handles tokenization, encoding, and the classification head automatically.\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# 2. Test it on real data (Supervised Learning Inference)\n",
    "test_reviews = [\n",
    "    \"This movie was an absolute masterpiece with brilliant acting.\",\n",
    "    \"I hated every minute of this film; the plot was a total disaster.\"\n",
    "]\n",
    "\n",
    "results = classifier(test_reviews)\n",
    "\n",
    "# 3. Print the results\n",
    "for review, result in zip(test_reviews, results):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Result: {result['label']} (Confidence: {result['score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6ouNKJj7ft9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN1Cms4aZLH/y+3wZXUnS3/",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
