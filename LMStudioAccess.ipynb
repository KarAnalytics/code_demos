{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarAnalytics/code_demos/blob/main/LMStudioAccess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THIS CODE WONT EXECUTE IN GOOGLE COLAB. You need to get lmstudio server running and then run this in VS Code or any other local IDE!**"
      ],
      "metadata": {
        "id": "rJjV47tPbfd1"
      },
      "id": "rJjV47tPbfd1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dfd31fa",
      "metadata": {
        "id": "4dfd31fa"
      },
      "outputs": [],
      "source": [
        "import lmstudio as lms\n",
        "## !pip install openai\n",
        "from openai import OpenAI\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebf1c11",
      "metadata": {
        "id": "3ebf1c11"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get the current working directory\n",
        "cwd = os.getcwd()\n",
        "print(\"Current Working Directory:\", cwd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "082d7a9c",
      "metadata": {
        "id": "082d7a9c",
        "outputId": "c2dbb1ab-7ff1-42d8-c355-a30ccd29059a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why donâ€™t scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Initialize the client pointing to your local LM Studio server\n",
        "client = OpenAI(\n",
        "    base_url=\"http://127.0.0.1:1234/v1\", # Ensure you add \"/v1\" to the URL\n",
        "    api_key=\"lm-studio\" # The SDK requires a key, but LM Studio ignores the specific value\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-oss-20B\", # Ensure this matches the model loaded in LM Studio\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
        "    ],\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf50461",
      "metadata": {
        "id": "bcf50461"
      },
      "source": [
        "Another example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e74a5c67",
      "metadata": {
        "id": "e74a5c67"
      },
      "outputs": [],
      "source": [
        "model = lms.llm()\n",
        "\n",
        "# 2. Write your prompt\n",
        "prompt = \"What is today's date?\"\n",
        "\n",
        "# 3. Get the response\n",
        "print(\"Awaiting response...\")\n",
        "result = model.respond(prompt)\n",
        "\n",
        "print(f\"\\nLLM Says: {result}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venvRCF2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}